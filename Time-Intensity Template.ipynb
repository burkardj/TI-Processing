{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Intensity measurments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "import random\n",
    "from numpy import mean\n",
    "import statistics as s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609830b",
   "metadata": {},
   "source": [
    "<u>The data from EyeQ are structured column-wise\n",
    "<ul>\n",
    "    <li> Product I Session I Replica I Sequence I Q1_Melting I Q2_1s I Q2_2s I ... I Q2_180s\n",
    "        <li> Melting of data based on Judge (Panelist), Product and Session = df_melt\n",
    "           \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = r'C:\\Users\\burka\\OneDrive\\Desktop\\global TI_.xlsx'\n",
    "df = pd.read_excel(root)\n",
    "\n",
    "##Do you want to delete unncesseray Columns such as Replica, Sequence or Comments##\n",
    "columns = list(df.columns.values) \n",
    "df = df.drop(columns = [\"Replica\",'Sequence',str(columns[-1])]) \n",
    "columns = list(df.columns.values) \n",
    "\n",
    "##COLUMN NAMES CHANGE ACCORDING TO TIME SEQUENCES##\n",
    "#time_interval = int(input(\"Was ist das Zeitintervall zwischen den Punkten?\"))\n",
    "time_interval = 1 ##Frequency of data Recording\n",
    "Pre_Int = False #If only certain frequency ranges\n",
    "Anz_Pre_Int = 3  #Number of pre-defined intervals\n",
    "Zeit_zw_Pre_Int = 10 #Time b/t pre-defined intervals\n",
    "non_int_col = 4 # number of columns before starting the time - when df Replica and Sequence is dropped\n",
    "decay_alt = True \n",
    "\n",
    "\n",
    "###APPROACH##\n",
    "#1 - RENAME ALL COLUMNS  \n",
    "#A.df - drop unnecessary columns B. df_melt for sns \n",
    "\n",
    "def list2(nomencl,targetarray):\n",
    "    for i in nomencl:\n",
    "        i = []\n",
    "        targetarray.append(i)\n",
    "\n",
    "        \n",
    "#Start column that needs to be changed is 4 - #RENAME ALL COLUMNS         \n",
    "column_new = []\n",
    "arr = df.columns.values\n",
    "\n",
    "if Pre_Int == True:\n",
    "    for i in range(Anz_Pre_Int):\n",
    "        k = (i+1)*Zeit_zw_Pre_Int #Interval Vordefiniert - Laufvariable\n",
    "        column_new.append(str(k))\n",
    "    for j in range(len(arr[non_int_col:])-Anz_Pre_Int):\n",
    "        k = (Anz_Pre_Int)*Zeit_zw_Pre_Int + (j+1) * time_interval\n",
    "        column_new.append(str(k))\n",
    "else:\n",
    "    for j in range(len(arr[non_int_col:])):\n",
    "        k = (j+1) * time_interval\n",
    "        column_new.append(str(k))\n",
    "    \n",
    "        \n",
    "arr = df.columns.values\n",
    "arr[non_int_col-1] = 'Stop Time'\n",
    "arr[non_int_col:] = column_new\n",
    "df3 = df.drop(columns = ['Stop Time']) ##needed for visualisation of Raw Data\n",
    "\n",
    "\n",
    "#RAW DATA Visualization#\n",
    "df_melt = pd.melt(df3, id_vars=[\"Judge\",\"Product\",\"Session\"]) #Herunterbrechen nach Variablen Judge,Product,Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a0475",
   "metadata": {},
   "source": [
    "<u>Raw Data Visualization using sns.catplot and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b44d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df_melt, # from your Dataframe\n",
    "                   col=\"Product\", # Make a subplot in columns for each variable in \"animal\"\n",
    "                   col_wrap=2, # Maximum number of columns per row \n",
    "                   x=\"variable\", # on x-axis make category on the variable \"variable\" (created by the melt operation)\n",
    "                   y=\"value\", # The corresponding y values\n",
    "                   hue=\"Judge\", # color according to the column gender\n",
    "                   kind=\"point\", # the kind of plot, the closest to what you want is a stripplot, \n",
    "                   legend_out=True, # let the legend inside the first subplot.\n",
    "                   )\n",
    "\n",
    "g.set_xlabels(\"Time\")\n",
    "g.set_ylabels(\"Intensity\")\n",
    "g.set_titles(template=\"{col_name}\") # otherwise it's \"animal = dog\", now it's just \"dog\"\n",
    "sns.despine(trim=True) # trim the axis.\n",
    "\n",
    "g.savefig('RawData.png')\n",
    "\n",
    "# EXPORT OF PLOT\n",
    "export = False\n",
    "\n",
    "if export == True:\n",
    "    #root = input(\"Please copy pace the path input, where you want to store your data\")\n",
    "    new_name = input('Please specify the name of your export data (dont forget .jpg):')\n",
    "    new_path = \"%s\" % ((root))  # Output\n",
    "    g.savefig('%s%s' %((new_path, new_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ba010",
   "metadata": {},
   "source": [
    "<u>Raw Data Visualization using sns.catplot and save\n",
    "<ul>\n",
    "    <li> create specific time-marks in new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = df\n",
    "                        \n",
    "tstart = []  \n",
    "Istart = []\n",
    "tmax = []\n",
    "Imax = []\n",
    "tasc = []\n",
    "Iasc = []\n",
    "tmelt = []\n",
    "tend = []\n",
    "Iend = []\n",
    "\n",
    "\n",
    "k = [[] for l in range(sf.shape[0])]\n",
    "\n",
    "for j in range(sf.shape[0]): #sf.shape[0] = Anz Reihen\n",
    "    #a-f sind nur Laufvariablen\n",
    "    transit = pd.to_numeric(sf.iloc[j,non_int_col:])\n",
    "    #MAX VALUE \n",
    "    a = sf.iloc[j,non_int_col:].max() #4 b/c four non int columns before\n",
    "    Imax.append(a)\n",
    "    b = transit.idxmax()\n",
    "    tmax.append(b)\n",
    "\n",
    "    #START\n",
    "    c = [n for n, i in enumerate(transit) if i>0][0]\n",
    "    tstart.append(sf.columns[c+non_int_col])\n",
    "    Istart.append(transit[c])\n",
    "\n",
    "    # DESCENDING LAST MAX VALUE OR FIRST TIME SINGLE MAX VALUE IS LEFT\n",
    "    if pd.Series(transit).value_counts().get(max(transit)) > 1:\n",
    "        d = [n for n, i in enumerate(transit) if i == max(transit)][-1] \n",
    "    else:\n",
    "        if decay_alt == False:\n",
    "            d = [n for n, i in enumerate(transit) if i == max(transit)][0]+1  ###THE +1 means appending 1\n",
    "        else:\n",
    "            d = [n for n, i in enumerate(transit) if i == max(transit)][0]\n",
    "    \n",
    "    tasc.append(sf.columns[d+non_int_col])\n",
    "    Iasc.append(transit[d])\n",
    "\n",
    "    #T-END --> if ZERO start conditioning\n",
    "    cond = True in (ele == 0 for ele in transit)\n",
    "    for i in range(sf.shape[1]-4):\n",
    "        if transit[i] == 0:\n",
    "            if i > int(tmax[j]):\n",
    "                k[j].append(i)\n",
    "    \n",
    "    if cond == True:\n",
    "        if  [n for n, i in enumerate(transit) if i == 0][0] != 0:\n",
    "            s = [n for n, i in enumerate(transit) if i == 0][0]\n",
    "            tend.append(sf.columns[s+non_int_col])\n",
    "        elif pd.Series(transit).value_counts().get(0) > 1:\n",
    "            s = [n for n, i in enumerate(transit) if i == 0][1]\n",
    "            tend.append(sf.columns[s+non_int_col])\n",
    "        else: ##NUR ALS NOTLOESUNG DA NIEMALS BEI 0 ---> kann später gelöscht werden\n",
    "            tend.append(160)\n",
    "    else: \n",
    "        tend.append(160)\n",
    "    \n",
    "    Iend = [0 for k in range(sf.shape[0])] #bis hier Notlösung   \n",
    "\n",
    "    #T-SCHMELZ ZEITPUNKT - ALWAYS CONTROL \n",
    "    tmelt.append(sf.iloc[j,3])\n",
    "\n",
    "\n",
    "tend = []\n",
    "for i in range(len(k)):\n",
    "    tend.append(float(k[i][0]))\n",
    "\n",
    "\n",
    "tstart = [float(x) for x in tstart]\n",
    "tmax = [float(x) for x in tmax]\n",
    "tasc = [float(x) for x in tasc]\n",
    "tmelt = [float(x) for x in tmelt]\n",
    "#tend = [float(x) for x in tend]\n",
    "\n",
    "sf_final = df.drop(df.iloc[:, non_int_col-1:], axis = 1)\n",
    "\n",
    "##COLUMN ERWEITERUNG##\n",
    "sf_final['tstart'] = tstart\n",
    "sf_final['Istart'] = Istart\n",
    "sf_final['tmax'] = tmax\n",
    "sf_final['Imax'] = Imax\n",
    "sf_final['tdesc'] = tasc\n",
    "sf_final['Idesc'] = Iasc\n",
    "sf_final['tmelt'] = tmelt\n",
    "sf_final['tend'] = tend\n",
    "sf_final['Iend'] = Iend \n",
    "\n",
    "liste = ['tstart_g','Istart_g','tmax_g','Imax_g','tdesc_g','Idesc_g','tmelt_g','tend_g','Iend_g']\n",
    "\n",
    "for n in range(len(liste)): #3 b/c of Judge,Product,Session\n",
    "    pend = [[] for x in range(len(sf['Product'].unique()))]\n",
    "    for i in range(sf.shape[0]):\n",
    "        for j in range(len(sf['Product'].unique())):\n",
    "            if sf.iloc[i,1] == sf['Product'].unique()[j]:\n",
    "                pend[j].append(sf_final.iloc[i,non_int_col-1+n])\n",
    "    sf_final[liste[n]] = pd.NaT\n",
    "    \n",
    "    for i in range(sf.shape[0]):\n",
    "        for j in range(len(sf['Product'].unique())):\n",
    "             if sf.iloc[i,1] == sf['Product'].unique()[j]:\n",
    "                    sf_final.iloc[i,-1] = mean(pend[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9aeea",
   "metadata": {},
   "source": [
    "<u>Normalization in Intensity (y-Axis) of Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 - Intensity Normed based on Imax,i and Imax,g for each product - in_df (and _melt)\n",
    "\n",
    "#in_df = intensity normed data frame \n",
    "in_df = {'Judge':[str(sf['Judge'].tolist()[i]) for i in range(sf.shape[0])],\n",
    "         'Product':[str(sf['Product'].tolist()[i]) for i in range(sf.shape[0])]}\n",
    "in_df = pd.DataFrame(in_df)\n",
    "\n",
    "columns = list(df.columns.values) #COLUMN NAMES FOR NEW DF 'First Values = 4'\n",
    "\n",
    "for j in range(len(columns)-non_int_col): #There the 15 and 6 are fixed from sf_final and independent on df above\n",
    "    in_df[columns[non_int_col+j]] = pd.NaT\n",
    "    for i in range(sf.shape[0]):\n",
    "        in_df.iloc[i,2+j] = float((sf.iloc[i,non_int_col+j] * sf_final.iloc[i,15]) / sf_final.iloc[i,6])\n",
    "\n",
    "       \n",
    "        \n",
    "#RAW DATA Visualization#\n",
    "in_df_melt = pd.melt(in_df, id_vars=[\"Judge\",\"Product\"]) #Herunterbrechen nach Variablen Judge,Product,Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d085b",
   "metadata": {},
   "source": [
    "<u>Normalization in Time-Axis (x-Axis) of Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4 - Normalize in_df also in t-direction to have product given same tstart,max,dec and end\n",
    "\n",
    "## rows = number of rows in df * number of time iterations \n",
    "last = ['Stop Time']\n",
    "num_time_it = list(df.columns.values).index(last[0]) ##equivalent to 4 in beginning  -1 \n",
    "\n",
    "## For the new df -> first 1Judge then through all time iterations, then next.....\n",
    "df_adapted_melt = {'Judge':[str(df['Judge'].tolist()[i]) for i in range(df.shape[0])\n",
    "                            for j in range(df.shape[1]-num_time_it-1)],\n",
    "                   'Product':[str(df['Product'].tolist()[i]) \n",
    "                              for i in range(df.shape[0]) for j in range(df.shape[1]-num_time_it-1)],\n",
    "                  'Session':[str(df['Session'].tolist()[i]) \n",
    "                              for i in range(df.shape[0]) for j in range(df.shape[1]-num_time_it-1)]}\n",
    "df_adapted_melt = pd.DataFrame(df_adapted_melt)\n",
    "\n",
    "\n",
    "## NOW NORMALIZE TIME \n",
    "\n",
    "## TWO WAYS OF ITERATING THROUGH DATA EITHER VIA COMPARING if Judge,Prod und Session = gleich, aber das benötigt \n",
    "# drei Laufvariablen, deshalb aufgrund der Strukturierung von oben wird Jede Kombi Judge,Prod und Session\n",
    "# chronologisch durchgegangen nachdem alle zeiten abgegangen sind\n",
    "\n",
    "tstarti = list(sf_final.columns.values).index('tstart')\n",
    "tmaxi = list(sf_final.columns.values).index('tmax')\n",
    "tdeci = list(sf_final.columns.values).index('tdesc') \n",
    "tendi = list(sf_final.columns.values).index('tend') \n",
    "tstartg = list(sf_final.columns.values).index('tstart_g')\n",
    "tmaxg = list(sf_final.columns.values).index('tmax_g')\n",
    "tdecg = list(sf_final.columns.values).index('tdesc_g')\n",
    "tendg = list(sf_final.columns.values).index('tend_g')\n",
    "\n",
    "time_list = list(df.columns.values)[num_time_it+1:]\n",
    "transit_t = []\n",
    "\n",
    "### TIME NORMALIZATION\n",
    "for j in range(sf.shape[0]):\n",
    "    for i in range(len(list(df.columns.values)[num_time_it+1:])):\n",
    "        if float(time_list[i]) <= sf_final.iloc[j,tmaxi]:  ### NORMALIZE ALL -MAX VALUE\n",
    "            if float(time_list[i]) < sf_final.iloc[j,tstarti]: ##Falls bspw 10 = time_list[i] < time_start \n",
    "                t = 0\n",
    "                transit_t.append(t)\n",
    "            elif sf_final.iloc[j,tmaxi] == sf_final.iloc[j,tstarti]:\n",
    "                t = sf_final.iloc[j,tmaxg]\n",
    "                transit_t.append(t)\n",
    "            else:\n",
    "                t = ((sf_final.iloc[j,tmaxg] - sf_final.iloc[j,tstartg]) / \n",
    "                     (sf_final.iloc[j,tmaxi] - sf_final.iloc[j,tstarti])) * (float(time_list[i]) - sf_final.iloc[j,tstarti]) + sf_final.iloc[j,tstartg]\n",
    "                transit_t.append(round(t,2))\n",
    "        elif sf_final.iloc[j,tmaxi] < int(time_list[i]) and int(time_list[i]) < sf_final.iloc[j,tdeci]: ### NORMALIZE ALL VALUES BT MAX AND DEC\n",
    "            t = ((sf_final.iloc[j,tdecg] - sf_final.iloc[j,tmaxg]) / \n",
    "                 (sf_final.iloc[j,tdeci] - sf_final.iloc[j,tmaxi])) * (float(time_list[i]) - sf_final.iloc[j,tmaxi]) + sf_final.iloc[j,tmaxg]\n",
    "            transit_t.append(round(t,2))\n",
    "        elif sf_final.iloc[j,tdeci] <= float(time_list[i]): ## NORMALIZE ALL OTHERS \n",
    "            t = ((sf_final.iloc[j,tendg] - sf_final.iloc[j,tdecg]) / \n",
    "                 (sf_final.iloc[j,tendi] - sf_final.iloc[j,tdeci])) * (float(time_list[i]) - sf_final.iloc[j,tdeci]) + sf_final.iloc[j,tdecg]\n",
    "            transit_t.append(round(t,2))\n",
    "\n",
    "\n",
    "df_adapted_melt['Time Normalized'] = transit_t\n",
    "df_adapted_melt['Int Normalized'] = pd.NaT\n",
    "transit_I = []\n",
    "\n",
    "for k in range(df.shape[0]): ##ADD INTENSITIES TO TIME\n",
    "    for l in range(len(list(df.columns.values)[num_time_it+1:])):\n",
    "        df_adapted_melt.iloc[l+k*len(list(df.columns.values)[num_time_it+1:]),4] = in_df.iloc[k,2 + l]  #2 because Judge/Product needs to be neglected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee85c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 - Create new x-axis dataframe on the basis of molten df \n",
    "#and corresponding intensities based on linear interpolation b/t nearest neighbours\n",
    "\n",
    "products = sorted(sf_final['Product'].unique())\n",
    "\n",
    "def globaltime(n):\n",
    "    itera =  ['tstart_g','tmax_g','tdesc_g','tend_g']\n",
    "    arr = [[] for k in range(len(products))]\n",
    "    for j in range(sf.shape[0]):\n",
    "        for i in range(len(products)):\n",
    "            if sf_final.iloc[j,1] == str(products[i]):\n",
    "                arr[i].append(sf_final[itera[n]][j])            \n",
    "    for i  in range(len(products)):\n",
    "        del arr[i][1:]\n",
    "    trans = []\n",
    "    for i in range(len(products)):\n",
    "        trans.append(float(arr[i][0]))\n",
    "    return trans\n",
    "\n",
    "start_mean = globaltime(0)\n",
    "max_mean = globaltime(1)\n",
    "dec_mean = globaltime(2)\n",
    "end_mean = globaltime(3)\n",
    "\n",
    "\n",
    "## NEW TIME RANGES DEFINE ##\n",
    "# r = max_m[i] - start_m[i], r2 = desc_m[i] - max_m[i] # nur 1L pro Produkt\n",
    "nu_int = 20\n",
    "\n",
    "\n",
    "def interval(anz_p): #with k index = number of product *0 = P01...\n",
    "    arr = [[] for m in range(anz_p)]\n",
    "    for m in range(anz_p):\n",
    "        for i in range(nu_int):\n",
    "            interval = max_mean[m] - start_mean[m]\n",
    "            add = round(interval * i/nu_int + start_mean[m],1) ##max_i - min_i * i & number of int + min_i\n",
    "            arr[m].append(add)\n",
    "        for i in range(nu_int):\n",
    "            interval = dec_mean[m] - max_mean[m]\n",
    "            add = round(interval * i/nu_int + max_mean[m],1)\n",
    "            arr[m].append(add)\n",
    "        for i in range(nu_int):\n",
    "            interval = end_mean[m] - dec_mean[m]\n",
    "            add = round(interval * i/nu_int + dec_mean[m],1)\n",
    "            arr[m].append(add) \n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def interval2(anz_p):\n",
    "    arr = [[] for m in range(anz_p)]\n",
    "    for m in range(anz_p):\n",
    "        for i in range(3*nu_int):\n",
    "            interval = end_mean[m] - start_mean[m]\n",
    "            add = round(interval * i/(3*nu_int) +start_mean[m],1)\n",
    "            arr[m].append(add)\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "## There new list is possibly defined\n",
    "if decay_alt == False:\n",
    "    t_int_neu = interval(len(products))\n",
    "else: \n",
    "    t_int_neu = interval2(len(products))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create corresponding list w/ talt, tneu siehe oben, Ialt und dann basierend darauf neue listen\n",
    "t_alt = df_adapted_melt['Time Normalized']\n",
    "t_alt = [round(float(t_alt[i]),1) for i in range(len(t_alt))] #numpy float to float \n",
    "I_alt = df_adapted_melt['Int Normalized']\n",
    "I_alt = [round(float(I_alt[i]),1) for i in range(len(I_alt))] #numpy float to float and round\n",
    "\n",
    "\n",
    "##PRINCIPLE OF INTERPOLATION BASED ON LINEAR INTERPOLATION B/T NEAREST NEIGHBOURS - \n",
    "#hence based on product (from df_adapted_melt) new lists with high low is created and always only with \n",
    "#the loop of  (len(time interval old))\n",
    "\n",
    "##FOR THE EQUATION to find new Int to tneu_int(i) \n",
    "## I'' = tneu_int(t) - thigh / (thigh-tlow) * (Ihigh- Ilow) + Ilow\n",
    "## if thigh = tlow -> I'' ? = Ilow \n",
    "## if thigh not existent --> what then ?\n",
    "\n",
    "def time_hoch(numberel):\n",
    "    t_alt = df_adapted_melt['Time Normalized']\n",
    "\n",
    "    time_high = [[] for m in range(sf.shape[0])]\n",
    "    for r in range(sf.shape[0]):\n",
    "        for i in range(len(products)):\n",
    "            t_neu = t_int_neu[i]\n",
    "            for k in range(len(t_neu)):\n",
    "                t_alt = [round(float(t_alt[j]),1) for j in range(len(t_alt))] #numpy float to float \n",
    "                t_alt_ = t_alt[0+numberel*r:numberel+numberel*r]\n",
    "                for m,l in enumerate(t_alt_):\n",
    "                    if df_adapted_melt.iloc[m+numberel*r,1] == str(products[i]):\n",
    "                        if l >= t_neu[k]:\n",
    "                            time_high[r].append(l)\n",
    "                            del time_high[r][1+k:] \n",
    "    return time_high\n",
    "\n",
    "\n",
    "time_list = list(df.columns.values)[num_time_it+1:]\n",
    "time_high = time_hoch(len(time_list))\n",
    "\n",
    "time_low = [[] for m in range(sf.shape[0])]\n",
    "trans = [[] for f in range(len(t_int_neu[0]))]\n",
    "for r in range(sf.shape[0]):\n",
    "    for i in range(len(products)):\n",
    "        t_neu = t_int_neu[i]\n",
    "        for k in range(len(t_neu)):\n",
    "            t_alt = [round(float(t_alt[i]),2) for i in range(len(t_alt))]\n",
    "            t_alt_ = t_alt[0+len(time_list)*r:len(time_list)+len(time_list)*r]\n",
    "            for m,l in enumerate(t_alt_):\n",
    "                if df_adapted_melt.iloc[m+len(time_list)*r,1] == str(products[i]):\n",
    "                    if l <= t_neu[k]:\n",
    "                        trans[k].append(l)\n",
    "                if len(trans[k]) > 1:\n",
    "                    del trans[k][:-1]\n",
    "    time_low[r] = [float(trans[i][0]) for i in range(len(t_int_neu[0]))]\n",
    "\n",
    "    \n",
    "int_high = [[] for k in range(sf.shape[0])]\n",
    "for k in range(sf.shape[0]):\n",
    "    time_high_ = time_high[k]\n",
    "    for j in range(len(time_high_)):\n",
    "        for i in range(len(time_list)):\n",
    "            if round(df_adapted_melt.iloc[i+len(time_list)*k][3],1) == time_high_[j]:\n",
    "                int_high[k].append(df_adapted_melt.iloc[i+len(time_list)*k][4])\n",
    "\n",
    "int_low = [[] for k in range(sf.shape[0])]\n",
    "for k in range(sf.shape[0]):\n",
    "    time_low_ = time_low[k]\n",
    "    for j in range(len(time_low_)):\n",
    "        for i in range(len(time_list)):\n",
    "            if round(df_adapted_melt.iloc[i+len(time_list)*k][3],1) == time_low_[j]:\n",
    "                int_low[k].append(df_adapted_melt.iloc[i+len(time_list)*k][4])\n",
    "   \n",
    "    \n",
    "### INTENSITY NORMALIZED\n",
    "## A. I'' = tneu_int(t) - thigh / (thigh-tlow) * (Ihigh- Ilow) + Ilow\n",
    "## B. if thigh = tlow -> I'' ? = Ilow \n",
    "## C. if thigh not existent --> then append values = last and then tlow = thigh -> then \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_final = []##FINAL INTENSITITES\n",
    "for k in range(sf.shape[0]):\n",
    "    for i in range(len(t_int_neu[0])):\n",
    "        if len(time_high[k]) < len(t_int_neu[0]):\n",
    "            while len(time_high[k]) < len(t_int_neu[0]): ##CASE C. always check to bring t_high to same level evy with the while\n",
    "                time_high[k].insert(-1,time_high[k][-1])\n",
    "        elif len(int_high[k]) < len(t_int_neu[0]):   \n",
    "            while len(int_high[k]) < len(t_int_neu[0]):\n",
    "                int_high[k].insert(-1,int_high[k][-1])\n",
    "        elif len(int_low[k]) < len(t_int_neu[0]):\n",
    "            while len(int_low[k]) < len(t_int_neu[0]):\n",
    "                int_low[k].insert(-1,int_low[k][-1])\n",
    "        for j in range(len(products)):\n",
    "            if df_adapted_melt.iloc[len(time_list)*k,1] == str(products[j]):\n",
    "                if time_high[k][i] == time_high[k][i]:\n",
    "                    intensity_final.append(int_low[k][i])\n",
    "                else:\n",
    "                    intensity_final.append(((t_int_neu[j][i]-time_low[k][i]) / (time_high[k][i] - time_low[k][i])) * \n",
    "                                           (int_high[k][i] - int_low[k][i])\n",
    "                              + int_low[k][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3758881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as s\n",
    "#6- create final molten df with normalized and time-scaled avg data \n",
    "#7 - take means / stdv \n",
    "\n",
    "\n",
    "## For the new df -> first 1Judge then through all time iterations, then next.....\n",
    "df_final_norm = {'Judge':[str(df['Judge'].tolist()[i]) for i in range(df.shape[0])\n",
    "                            for j in range(len(t_int_neu[0]))],\n",
    "                   'Product':[str(df['Product'].tolist()[i]) \n",
    "                              for i in range(df.shape[0]) for j in range(len(t_int_neu[0]))],\n",
    "                  'Session':[str(df['Session'].tolist()[i]) \n",
    "                              for i in range(df.shape[0]) for j in range(len(t_int_neu[0]))]}\n",
    "df_final_norm = pd.DataFrame(df_final_norm)\n",
    "df_final_norm['Time Normalized'] = pd.NaT\n",
    "                    \n",
    "liste = []\n",
    "\n",
    "for k in range(sf.shape[0]):\n",
    "    for i in range(len(t_int_neu[0])):\n",
    "        for j in range(len(products)):\n",
    "             if df_final_norm.iloc[i+len(t_int_neu[0])*k,1] == str(products[j]):\n",
    "                    liste.append(t_int_neu[j][i])\n",
    "\n",
    "df_final_norm['Time Normalized'] = liste\n",
    "df_final_norm['Intensity Normalized'] = intensity_final\n",
    "\n",
    "df_final_norm\n",
    "\n",
    "\n",
    "#create trans lists to average all data (incl stdv)\n",
    "trans = [[[] for j in range(len(products))] for m in range(len(t_int_neu[0]))]\n",
    "#len(t_int_neu[0])\n",
    "for k in range(sf.shape[0]):\n",
    "    for i in range(len(t_int_neu[0])):\n",
    "        for j in range(len(products)):\n",
    "            if df_final_norm.iloc[i+len(t_int_neu[0])*k,1] == str(products[j]):\n",
    "                if t_int_neu[j][i] == df_final_norm.iloc[i+len(t_int_neu[0])*k,3]:\n",
    "                    trans[i][j].append(df_final_norm.iloc[i+len(t_int_neu[0])*k,4])\n",
    "\n",
    "\n",
    "final_means = [[[] for j in range(len(products))] for m in range(len(t_int_neu[0]))]\n",
    "final_stdv = [[[] for j in range(len(products))] for m in range(len(t_int_neu[0]))]\n",
    "for i in range(len(t_int_neu[0])):\n",
    "    for j in range(len(products)):\n",
    "        final_means[i][j] = mean(trans[i][j])\n",
    "        final_stdv[i][j] = s.pstdev(trans[i][j])\n",
    "\n",
    "df_final_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Addition of 0 value at 0 in final_means[0][1] and [0][0]\n",
    "\n",
    "#import numpy as np\n",
    "#only if zero value integrated\n",
    "#final_means = np.insert(final_means, 0, 0, 0)\n",
    "#final_stdv = np.insert(final_stdv, 0, 0, 0)\n",
    "#t_int_neu[0].insert(0, 0)\n",
    "#t_int_neu[1].insert(0,0)\n",
    "lines_melt = [[] for k in range(len(sf_final['Product'].unique()))]\n",
    "for j in range(sf_final.shape[0]):\n",
    "    for i in range(len(sf_final['Product'].unique())):\n",
    "        if sorted(sf_final['Product'].unique())[i] == sf_final['Product'][j]:\n",
    "            lines_melt[i].append(sf_final['tmelt_g'][j])\n",
    "for i in range(len(sf_final['Product'].unique())):\n",
    "    lines_melt[i] = lines_melt[i][-1]\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "fig.suptitle(' Normierte Kurven ', fontsize=30) \n",
    "products = sorted(sf_final['Product'].unique())\n",
    "\n",
    "\n",
    "for i in range(len(products)):\n",
    "    trans_means = [[] for range in range(len(products))]\n",
    "    trans_stdv = [[] for range in range(len(products))]\n",
    "    for j in range(len(t_int_neu[0])):\n",
    "        trans_means[i].append(final_means[j][i])\n",
    "        trans_stdv[i].append(final_stdv[j][i])\n",
    "        \n",
    "    plt.subplot(4,2,i+1)\n",
    "    \n",
    "    plt.plot(t_int_neu[i],trans_means[i], '--bo', label ='line with marker')\n",
    "\n",
    "    #plt.scatter(t_int_neu[i], trans_means[i], color='b')\n",
    "    plt.axvline(x=float(lines_melt[i]))\n",
    "    plt.xlabel('t_norm')\n",
    "    plt.ylabel('I_norm')\n",
    "    plt.title('%s' %products[i], fontsize = 10) \n",
    "    #plt.errorbar(t_int_neu[i], a[i], yerr=b[i], fmt=\"o\", color=\"r\")  \n",
    "    fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('Normierte Kurven.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_mean = {'Product': [products[i]  for j in range(len(t_int_neu[0])) for i in range(len(products)) ],\n",
    "                'Time':[t_int_neu[j][i] for i in range(len(t_int_neu[0])) for j in range(len(products))],\n",
    "                 'Int Means':[final_means[i][j] for i in range(len(t_int_neu[0])) for j in range(len(products))],\n",
    "                 'Int Stdv':[final_stdv[i][j] for i in range(len(t_int_neu[0])) for j in range(len(products))]      \n",
    "                }\n",
    "\n",
    "df_final_mean = pd.DataFrame(df_final_mean)\n",
    "df_final_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### EXPORT OF DATA IN ONE EXCEL SHEET ###\n",
    "\n",
    "\n",
    "name = r'globalTI2_real_.xlsx'\n",
    "\n",
    "#df = RAW DATA\n",
    "#sf_final = AVERAGED NUMBER\n",
    "#in_df = NORMALIZED INTENSITY\n",
    "#df_adpated_melt = Normalized T - I molten and not time interval adjusted\n",
    "#df_final_normed \n",
    "#df_final_means\n",
    "\n",
    "# g.savefig('dummy.png')\n",
    "\n",
    "with pd.ExcelWriter('%s%s' %(root[:-10],name)) as writer:\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index\n",
    "    # to store the dataframe in specified sheet\n",
    "    df.to_excel(writer, sheet_name=\"Raw Data\", index=False)\n",
    "    #dummy.to_excel(writer,sheet_name='Img Raw Data',index=False)\n",
    "    #worksheet = writer.sheets['Raw Data']\n",
    "    #worksheet.insert_image('X2','RawData.png')\n",
    "    sf_final.to_excel(writer, sheet_name=\"Averaged Numbers\", index=False)\n",
    "    #in_df.to_excel(writer, sheet_name=\"Normalized Intensity\", index=False)\n",
    "    #worksheet = writer.sheets['Normalized Intensity']\n",
    "    #worksheet.insert_image('X2','NormalizedInt.png')\n",
    "    #df_adapted_melt.to_excel(writer, sheet_name ='Molten Norm T-I', index = False)\n",
    "    #worksheet = writer.sheets['Molten Norm T-I']\n",
    "    #worksheet.insert_image('G2','Molten_TimeInt.png')\n",
    "    #df_final_norm.to_excel(writer, sheet_name = 'Molten RAW data Normed',index = False)\n",
    "    df_final_mean.to_excel(writer,sheet_name = 'AVERAGED FINAL DATA',index =False)\n",
    "    worksheet = writer.sheets['AVERAGED FINAL DATA']\n",
    "    worksheet.insert_image('G2','Normierte Kurven.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
